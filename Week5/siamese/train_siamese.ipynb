{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training Siamese"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from siamese import Siamese\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "import pandas as pd\n",
    "from dataset import CarDataset\n",
    "import sys\n",
    "\n",
    "cuda = False\n",
    "way = 20\n",
    "times = 400\n",
    "workers = 4\n",
    "bacth_size = 128\n",
    "lr = 0.00006\n",
    "show_every = 10\n",
    "save_every = 100\n",
    "test_every = 100\n",
    "max_iter = 50_000\n",
    "model_path = \"\"\n",
    "gpu_ids = \"0\"\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 38074 Test size 6174\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../patches/gt_car_patches_annotations.csv', delimiter=',')\n",
    "\n",
    "train = dataset[(dataset.SEQ == 'S01') | (dataset.SEQ == 'S04')]\n",
    "test = dataset[(dataset.SEQ == 'S03')]\n",
    "\n",
    "print(f\"Train size: {train.shape[0]} Test size {test.shape[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu: 0 to train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmartinfernandez/opt/anaconda3/envs/M6_Video/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x1000 and 9216x4096)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 30>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     36\u001B[0m     img1, img2, label \u001B[38;5;241m=\u001B[39m Variable(img1), Variable(img2), Variable(label)\n\u001B[1;32m     38\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 39\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(output, label)\n\u001B[1;32m     41\u001B[0m loss_val \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/SharedFolder/M6_Video/Week5/siamese/siamese.py:34\u001B[0m, in \u001B[0;36mSiamese.forward\u001B[0;34m(self, x1, x2)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x1, x2):\n\u001B[0;32m---> 34\u001B[0m \tout1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m \tout2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_one(x2)\n\u001B[1;32m     36\u001B[0m \tdis \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mabs(out1 \u001B[38;5;241m-\u001B[39m out2)\n",
      "File \u001B[0;32m~/SharedFolder/M6_Video/Week5/siamese/siamese.py:30\u001B[0m, in \u001B[0;36mSiamese.forward_one\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     28\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv(x)\n\u001B[1;32m     29\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mliner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/M6_Video/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/M6_Video/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 141\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/M6_Video/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/M6_Video/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (128x1000 and 9216x4096)"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids\n",
    "print(\"use gpu:\", gpu_ids, \"to train.\")\n",
    "\n",
    "trainSet = CarDataset(train, transform=data_transforms)\n",
    "testSet = CarDataset(test, transform=data_transforms)\n",
    "\n",
    "testLoader = DataLoader(testSet, batch_size=way, shuffle=False, num_workers=workers)\n",
    "trainLoader = DataLoader(trainSet, batch_size=bacth_size, shuffle=False, num_workers=workers)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=True)\n",
    "net = Siamese()\n",
    "\n",
    "# multi gpu\n",
    "if len(gpu_ids.split(\",\")) > 1:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "\n",
    "if cuda:\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "train_loss = []\n",
    "loss_val = 0\n",
    "time_start = time.time()\n",
    "queue = deque(maxlen=20)\n",
    "\n",
    "for batch_id, (img1, img2, label) in enumerate(trainLoader, 1):\n",
    "    if batch_id > max_iter:\n",
    "        break\n",
    "    if cuda:\n",
    "        img1, img2, label = Variable(img1.cuda()), Variable(img2.cuda()), Variable(label.cuda())\n",
    "    else:\n",
    "        img1, img2, label = Variable(img1), Variable(img2), Variable(label)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = net.forward(img1, img2)\n",
    "    loss = loss_fn(output, label)\n",
    "    loss_val += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_id % show_every == 0 :\n",
    "        print('[%d]\\tloss:\\t%.5f\\ttime lapsed:\\t%.2f s'%(batch_id, loss_val/show_every, time.time() - time_start))\n",
    "        loss_val = 0\n",
    "        time_start = time.time()\n",
    "    if batch_id % save_every == 0:\n",
    "        torch.save(net.state_dict(), model_path + '/model-inter-' + str(batch_id+1) + \".pt\")\n",
    "\n",
    "    if batch_id % test_every == 0:\n",
    "        right, error = 0, 0\n",
    "        for _, (test1, test2) in enumerate(testLoader, 1):\n",
    "            if cuda:\n",
    "                test1, test2 = test1.cuda(), test2.cuda()\n",
    "\n",
    "            test1, test2 = Variable(test1), Variable(test2)\n",
    "            output = net.forward(test1, test2).data.cpu().numpy()\n",
    "            pred = np.argmax(output)\n",
    "            if pred == 0:\n",
    "                right += 1\n",
    "            else: error += 1\n",
    "\n",
    "        print('*'*70)\n",
    "        print('[%d]\\tTest set\\tcorrect:\\t%d\\terror:\\t%d\\tprecision:\\t%f'%(batch_id, right, error, right*1.0/(right+error)))\n",
    "        print('*'*70)\n",
    "        queue.append(right*1.0/(right+error))\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    train_loss.append(loss_val)\n",
    "#  learning_rate = learning_rate * 0.95\n",
    "\n",
    "with open('train_loss', 'wb') as f:\n",
    "    pickle.dump(train_loss, f)\n",
    "\n",
    "acc = 0.0\n",
    "for d in queue:\n",
    "    acc += d\n",
    "print(\"#\"*70)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}