{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training Siamese"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from pytorch_metric_learning import losses\n",
    "from siamese import Siamese\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "import pandas as pd\n",
    "from dataset import CarDataset\n",
    "import sys\n",
    "\n",
    "cuda = False\n",
    "way = 20\n",
    "times = 400\n",
    "workers = 4\n",
    "bacth_size = 128\n",
    "lr = 0.00006\n",
    "show_every = 10\n",
    "save_every = 100\n",
    "test_every = 100\n",
    "max_iter = 50_000\n",
    "gpu_ids = \"0\"\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 38074 Test size 6174\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../patches/gt_car_patches_annotations.csv', delimiter=',')\n",
    "\n",
    "train = dataset[(dataset.SEQ == 'S01') | (dataset.SEQ == 'S04')]\n",
    "test = dataset[(dataset.SEQ == 'S03')]\n",
    "\n",
    "print(f\"Train size: {train.shape[0]} Test size {test.shape[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define the Contrastive Loss Function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "      # Calculate the euclidean distance and calculate the contrastive loss\n",
    "      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "\n",
    "      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "      return loss_contrastive\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu: 0 to train.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "labels must be a 1D tensor of shape (batch_size,)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 31>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     39\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     40\u001B[0m output \u001B[38;5;241m=\u001B[39m net\u001B[38;5;241m.\u001B[39mforward(img1, img2)\n\u001B[0;32m---> 41\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m loss_val \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     43\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/M6_Video/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/M6_Video/lib/python3.9/site-packages/pytorch_metric_learning/losses/base_metric_loss_function.py:34\u001B[0m, in \u001B[0;36mBaseMetricLossFunction.forward\u001B[0;34m(self, embeddings, labels, indices_tuple, ref_emb, ref_labels)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124;03m    embeddings: tensor of size (batch_size, embedding_size)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03mReturns: the loss\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_stats()\n\u001B[0;32m---> 34\u001B[0m \u001B[43mc_f\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcheck_shapes\u001B[49m\u001B[43m(\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m labels \u001B[38;5;241m=\u001B[39m c_f\u001B[38;5;241m.\u001B[39mto_device(labels, embeddings)\n\u001B[1;32m     36\u001B[0m ref_emb, ref_labels \u001B[38;5;241m=\u001B[39m c_f\u001B[38;5;241m.\u001B[39mset_ref_emb(embeddings, labels, ref_emb, ref_labels)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/M6_Video/lib/python3.9/site-packages/pytorch_metric_learning/utils/common_functions.py:419\u001B[0m, in \u001B[0;36mcheck_shapes\u001B[0;34m(embeddings, labels)\u001B[0m\n\u001B[1;32m    415\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    416\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membeddings must be a 2D tensor of shape (batch_size, embedding_size)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    417\u001B[0m     )\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 419\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels must be a 1D tensor of shape (batch_size,)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: labels must be a 1D tensor of shape (batch_size,)"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids\n",
    "print(\"use gpu:\", gpu_ids, \"to train.\")\n",
    "\n",
    "trainSet = CarDataset(train, transform=data_transforms)\n",
    "testSet = CarDataset(test, transform=data_transforms)\n",
    "\n",
    "testLoader = DataLoader(testSet, batch_size=way, shuffle=False, num_workers=workers)\n",
    "trainLoader = DataLoader(trainSet, batch_size=bacth_size, shuffle=False, num_workers=workers)\n",
    "\n",
    "#loss_fn = torch.nn.BCEWithLogitsLoss(size_average=True)\n",
    "loss_fn = losses.ContrastiveLoss()\n",
    "net = Siamese()\n",
    "\n",
    "# multi gpu\n",
    "if len(gpu_ids.split(\",\")) > 1:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "\n",
    "if cuda:\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "train_loss = []\n",
    "loss_val = 0\n",
    "time_start = time.time()\n",
    "queue = deque(maxlen=20)\n",
    "\n",
    "for batch_id, (img1, img2, label) in enumerate(trainLoader, 1):\n",
    "\n",
    "    if batch_id > max_iter:\n",
    "        break\n",
    "\n",
    "    if cuda:\n",
    "        img1, img2, label = Variable(img1.cuda()), Variable(img2.cuda()), Variable(label.cuda())\n",
    "    else:\n",
    "        img1, img2, label = Variable(img1), Variable(img2), Variable(label)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = net.forward(img1, img2)\n",
    "    loss = loss_fn(output, label)\n",
    "    loss_val += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_id % show_every == 0 :\n",
    "        print('[%d]\\tloss:\\t%.5f\\ttime lapsed:\\t%.2f s'%(batch_id, loss_val/show_every, time.time() - time_start))\n",
    "        loss_val = 0\n",
    "        time_start = time.time()\n",
    "    if batch_id % save_every == 0:\n",
    "        torch.save(net.state_dict(), 'model-inter-' + str(batch_id+1) + \".pt\")\n",
    "\n",
    "    if batch_id % test_every == 0:\n",
    "        right, error = 0, 0\n",
    "        for _, (test1, test2) in enumerate(testLoader, 1):\n",
    "            if cuda:\n",
    "                test1, test2 = test1.cuda(), test2.cuda()\n",
    "\n",
    "            test1, test2 = Variable(test1), Variable(test2)\n",
    "            output = net.forward(test1, test2).data.cpu().numpy()\n",
    "            pred = np.argmax(output)\n",
    "            if pred == 0:\n",
    "                right += 1\n",
    "            else: error += 1\n",
    "\n",
    "        print('*'*70)\n",
    "        print('[%d]\\tTest set\\tcorrect:\\t%d\\terror:\\t%d\\tprecision:\\t%f'%(batch_id, right, error, right*1.0/(right+error)))\n",
    "        print('*'*70)\n",
    "        queue.append(right*1.0/(right+error))\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    train_loss.append(loss_val)\n",
    "#  learning_rate = learning_rate * 0.95\n",
    "\n",
    "with open('train_loss', 'wb') as f:\n",
    "    pickle.dump(train_loss, f)\n",
    "\n",
    "acc = 0.0\n",
    "for d in queue:\n",
    "    acc += d\n",
    "print(\"#\"*70)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}